---
{"dg-publish":true,"permalink":"/czc知识库/计算机/坤器学习/transformer/","dgPassFrontmatter":true,"created":"2024-08-22T20:15:50.614+08:00","updated":"2024-12-08T17:30:43.403+08:00"}
---


结构：
![](/img/user/czc知识库/杂七杂八/9-附件/附件/transformer_image-1.png))

transformer与RNN相比，更适合GPU并行

模型：是由公式和参数组成的
	chatGPT3.5有1350亿个参数，训练前都是一个随机数，训练完后就是一个具体的固定的参数了

注意力机制
注意力机制：如果现在有个人，他的身高是178cm，请预估他的体重

transformer是多头自注意力机制


AGI
越往AGI发展，算法工程师越不重要，


---

模型微调：第三阶技术：Fine-tune（定制垂直领域的大模型）（openai 是 Fine-tune的鼻祖）

1、80个顶级专业领域博士，一问一答，整理结合gpt3做一轮监督学习，作为微调（教大模型专业知识）
2、又找了一堆人，普通人，蹲在这给gpt提问，每提一个问，gpt给出4个答案，人给4个答案排序，一直重复。弄出一个积分系统（调教大模型）
3、就人类反馈的数据再学一次（RL）
4、再回到第二步再做
5、再


所谓基于人类反馈的强化学习